{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T13:26:24.723456Z",
     "start_time": "2020-07-22T13:25:49.169483Z"
    },
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from builtins import range, map, zip, filter\n",
    "from io import open\n",
    "import six,imp,os,sys\n",
    "from six.moves import cPickle\n",
    "import numpy as np \n",
    "# import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.datasets import mnist,cifar10,fashion_mnist\n",
    "from keras.models import Sequential,Model,load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten,Conv1D, Conv2D, Convolution2D, MaxPooling1D,MaxPooling2D,GlobalAveragePooling2D\n",
    "from keras.layers import CuDNNGRU,CuDNNLSTM,LSTM\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications import DenseNet121, DenseNet169, DenseNet201,VGG16,ResNet50,InceptionV3,imagenet_utils\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from keras.utils import plot_model\n",
    "from keras.losses import MAE\n",
    "from keras.optimizers import SGD, Adam, adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow import set_random_seed\n",
    "from PIL import Image\n",
    "from numpy.random import seed\n",
    "# seed(639)\n",
    "# set_random_seed(5944)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# h5转Bson格式保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T13:33:47.919034Z",
     "start_time": "2020-07-22T13:33:42.747103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From f:\\develop\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "{'layer_name': 'dense_1', 'op_type': 'Dense', 'input_name': 'dense_1_input:0', 'output_name': 'dense_1/Relu:0', 'input_shape': [784], 'output_shape': [100], 'units': 100, 'use_bias': True, 'activation': 'relu', 'weights_shape': [100, 784]}\n",
      "{'layer_name': 'dense_2', 'op_type': 'Dense', 'input_name': 'dense_1/Relu:0', 'output_name': 'dense_2/Relu:0', 'input_shape': [100], 'output_shape': [100], 'units': 100, 'use_bias': True, 'activation': 'relu', 'weights_shape': [100, 100]}\n",
      "{'layer_name': 'dense_3', 'op_type': 'Dense', 'input_name': 'dense_2/Relu:0', 'output_name': 'dense_3/Relu:0', 'input_shape': [100], 'output_shape': [10], 'units': 10, 'use_bias': True, 'activation': 'relu', 'weights_shape': [10, 100]}\n",
      "export successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 保证所有数据能够显示，而不是用省略号表示，np.inf表示一个足够大的数\n",
    "# np.set_printoptions(threshold = np.inf) \n",
    "# 若想不以科学计数显示:\n",
    "# np.set_printoptions(suppress = True)\n",
    "# np.set_printoptions(linewidth = np.inf)\n",
    "\n",
    "import bson,re\n",
    "class Convert2MyTools:\n",
    "    dt = dict()\n",
    "    layers = []\n",
    "    def extract_con2d(self,data,layer):\n",
    "        obj = dict()\n",
    "        obj['layer_name'] = layer.name\n",
    "        obj['op_type'] = type(layer).__name__\n",
    "        obj['input_name'] = layer.input.name\n",
    "        obj['output_name'] = layer.output.name\n",
    "        obj['input_shape'] = list(map(int,layer.input_shape[1:4]))\n",
    "        obj['output_shape'] = list(map(int,layer.output_shape[1:4]))\n",
    "        obj['filters'] = layer.filters\n",
    "        obj['kernel_size'] = list(layer.kernel_size)\n",
    "        obj['strides'] = list(layer.strides)\n",
    "        #   tf中padding=='same'表示全填充,valid表示不填充，padding=0表示不填充,padding=1四周都填充\n",
    "        obj['padding'] = 1 if layer.padding.lower()=='same' else 0\n",
    "        obj['use_bias'] = layer.use_bias\n",
    "        obj['activation'] = layer.activation.__name__\n",
    "        weights = layer.get_weights()\n",
    "        ws = weights[0]\n",
    "        # tf中的纬度是h,w,inchannel,outchannel，在内部会根据批次转换成NHWC\n",
    "        # Julia中计算是H W IC OC，julia中reshape方向相反,,python纬度转换变成(outchannel,inchannel,w,h) 拉平导出\n",
    "        (filter_height, filter_width, in_channels, out_channels) = ws.shape\n",
    "        obj['filters_shape'] = list(map(int,ws.shape)) # 这里的过滤器shape是为了后续的weights转换所用\n",
    "        obj['filter_height'] = filter_height\n",
    "        obj['filter_width'] = filter_width\n",
    "        obj['in_channels'] = in_channels\n",
    "        obj['out_channels'] = out_channels\n",
    "        print(obj)\n",
    "        obj['weights'] = ws.transpose(3,2,1,0).flatten().tolist()\n",
    "        obj['bias'] = []\n",
    "        if layer.use_bias:\n",
    "            obj['bias'] = weights[1].tolist()\n",
    "        data.append(obj)\n",
    "\n",
    "    def extract_max_avg_pooling(self,data,layer):\n",
    "        obj = dict()\n",
    "        obj['layer_name'] = layer.name\n",
    "        obj['op_type'] = type(layer).__name__\n",
    "        obj['input_name'] = layer.input.name\n",
    "        obj['output_name'] = layer.output.name\n",
    "        obj['pool_size'] = list(layer.pool_size)\n",
    "        obj['strides'] = list(layer.strides)\n",
    "        obj['padding'] = 1 if layer.padding.lower()=='same' else 0\n",
    "        obj['input_shape'] = list(map(int,layer.input_shape[1:4]))\n",
    "        obj['output_shape'] = list(map(int,layer.output_shape[1:4]))\n",
    "        obj['activation'] = 'linear'\n",
    "        print(obj)\n",
    "        data.append(obj)\n",
    "        \n",
    "    def extract_global_max_avg_pooling(self,data,layer):\n",
    "        obj = dict()\n",
    "        obj['layer_name'] = layer.name\n",
    "        obj['op_type'] = type(layer).__name__\n",
    "        obj['input_name'] = layer.input.name\n",
    "        obj['output_name'] = layer.output.name\n",
    "        obj['input_shape'] = list(map(int,layer.input_shape[1:4]))\n",
    "        obj['output_shape'] = list(map(int,layer.output_shape[1:4]))\n",
    "        obj['activation'] = 'linear'\n",
    "        print(obj)\n",
    "        data.append(obj)\n",
    "\n",
    "    def extract_flatten(self,data,layer):\n",
    "        obj = dict()\n",
    "        obj['layer_name'] = layer.name\n",
    "        obj['op_type'] = type(layer).__name__\n",
    "        obj['input_name'] = layer.input.name\n",
    "        obj['output_name'] = layer.output.name\n",
    "        obj['input_shape'] = list(map(int,layer.input_shape[1:4]))\n",
    "        obj['output_shape'] = list(map(int,layer.output_shape[1:4]))\n",
    "        obj['activation'] = 'linear'\n",
    "        print(obj)\n",
    "        data.append(obj)\n",
    "\n",
    "    def extract_dense(self,data,layer):\n",
    "        weights = layer.get_weights()\n",
    "        obj = dict()\n",
    "        obj['layer_name'] = layer.name\n",
    "        obj['op_type'] = type(layer).__name__\n",
    "        obj['input_name'] = layer.input.name\n",
    "        obj['output_name'] = layer.output.name\n",
    "        obj['input_shape'] = list(map(int,layer.input_shape[1:4]))\n",
    "        obj['output_shape'] = list(map(int,layer.output_shape[1:4]))\n",
    "        obj['units'] = layer.units\n",
    "        obj['use_bias'] = layer.use_bias\n",
    "#         if \"activation\" in layer.__dict__.items(): layer.has_key(\"activation\")\n",
    "        obj['activation'] = layer.activation.__name__\n",
    "        # 由于weights[0][:,i]是一列作为特征进行相乘，所以横向拉平后的数据在julia中对应的shape需要转置\n",
    "        obj['weights_shape'] = list(map(int,weights[0].shape))\n",
    "        obj['weights_shape'].reverse()\n",
    "        print(obj)\n",
    "        obj['weights'] = weights[0].flatten().tolist()\n",
    "        obj['bias'] = []\n",
    "        if layer.use_bias:\n",
    "            obj['bias'] = weights[1].tolist()\n",
    "        data.append(obj)\n",
    "     \n",
    "    def extract_zeropadding(self,data,layer):\n",
    "        obj = dict()\n",
    "        obj['layer_name'] = layer.name\n",
    "        obj['op_type'] = type(layer).__name__\n",
    "        obj['input_name'] = layer.input.name\n",
    "        obj['output_name'] = layer.output.name\n",
    "        obj['input_shape'] = list(map(int,layer.input_shape[1:4]))\n",
    "        obj['output_shape'] = list(map(int,layer.output_shape[1:4]))\n",
    "        obj['padding'] = np.array(layer.padding).flatten().tolist()\n",
    "        obj['activation'] = 'linear'\n",
    "        print(obj)\n",
    "        data.append(obj)\n",
    "    \n",
    "    def extract_batchnorm(self,data,layer):\n",
    "        weights = layer.get_weights()\n",
    "        obj = dict()\n",
    "        obj['layer_name'] = layer.name\n",
    "        obj['op_type'] = type(layer).__name__\n",
    "        obj['input_name'] = layer.input.name\n",
    "        obj['output_name'] = layer.output.name\n",
    "        obj['input_shape'] = list(map(int,layer.input_shape[1:4]))\n",
    "        obj['output_shape'] = list(map(int,layer.output_shape[1:4]))\n",
    "        obj['momentum'] = float(layer.momentum)\n",
    "        obj['epsilon'] = float(layer.epsilon)\n",
    "        obj['center'] = layer.center\n",
    "        obj['scale'] = layer.scale\n",
    "        print(obj)\n",
    "        obj['gamma'] = weights[0].tolist()\n",
    "        obj['beta'] = weights[1].tolist()\n",
    "        obj['moving_mean'] = weights[2].tolist()\n",
    "        obj['moving_variance'] = weights[3].tolist()\n",
    "        obj['activation'] = 'linear'\n",
    "        data.append(obj)\n",
    "        \n",
    "    def extract_activation(self,data,layer):\n",
    "        obj = dict()\n",
    "        obj['layer_name'] = layer.name\n",
    "        obj['op_type'] = type(layer).__name__\n",
    "        obj['input_name'] = layer.input.name\n",
    "        obj['output_name'] = layer.output.name\n",
    "        obj['input_shape'] = list(map(int,layer.input_shape[1:4]))\n",
    "        obj['output_shape'] = list(map(int,layer.output_shape[1:4]))\n",
    "        obj['activation'] = layer.activation.__name__\n",
    "        print(obj)\n",
    "        data.append(obj)\n",
    "        \n",
    "    def extract_add_or_concatenate(self,data,layer):\n",
    "        obj = dict()\n",
    "        obj['layer_name'] = layer.name\n",
    "        obj['op_type'] = type(layer).__name__\n",
    "        obj['output_name'] = layer.output.name\n",
    "        obj['output_shape'] = list(map(int,layer.output_shape[1:4]))\n",
    "        obj['activation'] = 'linear'\n",
    "        obj['input_names'] = []\n",
    "        obj['input_shapes'] = []\n",
    "        for _input in layer.input:\n",
    "            obj['input_names'].append(_input.name)\n",
    "            obj['input_shapes'].append(_input.shape.as_list()[1:4])\n",
    "        print(obj)\n",
    "        data.append(obj)\n",
    "        \n",
    "    def extract_dropout(self,data,layer):\n",
    "        obj = dict()\n",
    "        obj['layer_name'] = layer.name\n",
    "        obj['op_type'] = type(layer).__name__\n",
    "        obj['input_name'] = layer.input.name\n",
    "        obj['output_name'] = layer.output.name\n",
    "        obj['input_shape'] = list(map(int,layer.input_shape[1:4]))\n",
    "        obj['output_shape'] = list(map(int,layer.output_shape[1:4]))\n",
    "        obj['rate'] = float(layer.rate)\n",
    "        obj['activation'] = 'linear'\n",
    "        print(obj)\n",
    "        data.append(obj)\n",
    "        \n",
    "    global gd\n",
    "    def extract_model(self,model,path):\n",
    "        with open(path, 'wb') as fs:\n",
    "            data=[]\n",
    "            for index,layer in enumerate(model.layers):\n",
    "                layer_name = type(layer).__name__\n",
    "                if 'Conv2D' in layer_name:\n",
    "                    self.extract_con2d(data,layer)\n",
    "                elif 'Dense' in layer_name:\n",
    "                    self.extract_dense(data,layer)\n",
    "                elif 'Flatten' in layer_name:\n",
    "                    self.extract_flatten(data,layer)\n",
    "                elif 'Dropout' in layer_name:\n",
    "                    self.extract_dropout(data,layer)\n",
    "                elif 'Activation' in layer_name:\n",
    "                    self.extract_activation(data,layer)\n",
    "                elif 'ZeroPadding2D' in layer_name:\n",
    "                    self.extract_zeropadding(data,layer)\n",
    "                elif 'BatchNormalization' in layer_name:\n",
    "                    self.extract_batchnorm(data,layer) \n",
    "                elif layer_name in ['Add','Concatenate']:\n",
    "                    self.extract_add_or_concatenate(data,layer)\n",
    "                elif layer_name in ['MaxPooling2D','AveragePooling2D']:\n",
    "                    self.extract_max_avg_pooling(data,layer)\n",
    "                elif layer_name in ['GlobalMaxPooling2D','GlobalAveragePooling2D']:\n",
    "                    self.extract_global_max_avg_pooling(data,layer)\n",
    "                elif 'InputLayer' in layer_name:\n",
    "                    continue\n",
    "                else:\n",
    "                    print('not support layer',layer.name)\n",
    "            fs.write(bson.dumps({'data':data}))\n",
    "            print('export successfully!')\n",
    "\n",
    "\n",
    "model = load_model('./eran_3_100.h5')\n",
    "\n",
    "a = Convert2MyTools()\n",
    "a.extract_model(model,'eran_3_100.bson')\n",
    "\n",
    "\n",
    "# layer = model.layers[8]\n",
    "# print(type(layer).__name__)\n",
    "# print(layer.input_shape)\n",
    "# print(layer.output_shape)\n",
    "# weights = layer.get_weights()\n",
    "# ws = weights[0].transpose(3,2,1,0).flatten()\n",
    "# print(ws[:,:,0,0])\n",
    "# w_t = ws.transpose(3,2,1,0).flatten().tolist()\n",
    "# print(w_t)\n",
    "# a = re.sub(r'(]\\n)', '];',w_t)\n",
    "# a = re.sub(r'\\];[\\s]*\\[', '];[',a)\n",
    "# for layer in model.layers:\n",
    "#     print(layer.name)\n",
    "# for item in layer.__dict__.items():\n",
    "#     print(item)\n",
    "# with open('tt.bson', 'wb') as fs:\n",
    "#     p = dict()\n",
    "#     p[\"data\"]= ws.tolist()\n",
    "#     fs.write(bson.dumps(p))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T14:05:51.359261Z",
     "start_time": "2020-07-22T14:05:51.349253Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessing_data():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    print(x_train.shape)\n",
    "     # 归一化\n",
    "    x_train = x_train / 255\n",
    "    x_test = x_test / 255\n",
    "    x_train = x_train.reshape(60000,784) # 将图片摊平，变成向量\n",
    "    x_test = x_test.reshape(10000,784) # 对测试集进行同样的处理\n",
    "    # 标签激活\n",
    "    y_train = keras.utils.to_categorical(y_train, 10)\n",
    "    y_test = keras.utils.to_categorical(y_test, 10)\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T14:06:43.318084Z",
     "start_time": "2020-07-22T14:06:40.747423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                110       \n",
      "=================================================================\n",
      "Total params: 8,400\n",
      "Trainable params: 8,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=10,activation='relu',input_shape=(784,)))\n",
    "model.add(Dense(units=10,activation='relu'))\n",
    "model.add(Dense(units=10,activation='relu'))\n",
    "model.add(Dense(units=10,activation='relu'))\n",
    "model.add(Dense(units=10,activation='relu'))\n",
    "model.add(Dense(units=10,activation='softmax'))\n",
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T14:06:46.702073Z",
     "start_time": "2020-07-22T14:06:46.050525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ff5c7a1008fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'train'"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = preprocessing_data()\n",
    "model.fit(x_train, y_train,x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
