{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T05:22:31.940663Z",
     "start_time": "2020-08-04T05:22:30.960260Z"
    },
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from builtins import range, map, zip, filter\n",
    "from io import open\n",
    "import six,imp,os,sys\n",
    "from six.moves import cPickle\n",
    "import numpy as np \n",
    "# import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.datasets import mnist,cifar10,fashion_mnist\n",
    "from keras.models import Sequential,Model,load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten,Conv1D, Conv2D, Convolution2D, MaxPooling1D,MaxPooling2D,GlobalAveragePooling2D\n",
    "from keras.layers import CuDNNGRU,CuDNNLSTM,LSTM\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications import DenseNet121, DenseNet169, DenseNet201,VGG16,ResNet50,InceptionV3,imagenet_utils\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from keras.utils import plot_model\n",
    "from keras.losses import MAE\n",
    "from keras.optimizers import SGD, Adam, adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow import set_random_seed\n",
    "from PIL import Image\n",
    "from numpy.random import seed\n",
    "# seed(639)\n",
    "# set_random_seed(5944)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# h5转Bson格式保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T05:35:58.057503Z",
     "start_time": "2020-08-04T05:35:57.482557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_name': 'conv2d_7', 'op_type': 'Conv2D', 'input_name': 'conv2d_7_input:0', 'output_name': 'conv2d_7/Relu:0', 'input_shape': [28, 28, 1], 'output_shape': [14, 14, 16], 'filters': 16, 'kernel_size': [4, 4], 'strides': [2, 2], 'padding': 1, 'use_bias': True, 'activation': 'relu', 'filters_shape': [4, 4, 1, 16], 'filter_height': 4, 'filter_width': 4, 'in_channels': 1, 'out_channels': 16}\n",
      "{'layer_name': 'conv2d_8', 'op_type': 'Conv2D', 'input_name': 'conv2d_7/Relu:0', 'output_name': 'conv2d_8/Relu:0', 'input_shape': [14, 14, 16], 'output_shape': [7, 7, 32], 'filters': 32, 'kernel_size': [4, 4], 'strides': [2, 2], 'padding': 1, 'use_bias': True, 'activation': 'relu', 'filters_shape': [4, 4, 16, 32], 'filter_height': 4, 'filter_width': 4, 'in_channels': 16, 'out_channels': 32}\n",
      "{'layer_name': 'permute_4', 'op_type': 'Permute', 'input_name': 'conv2d_8/Relu:0', 'output_name': 'permute_4/transpose:0', 'input_shape': [7, 7, 32], 'output_shape': [32, 7, 7], 'dims': [3, 1, 2], 'activation': 'linear'}\n",
      "{'layer_name': 'flatten_4', 'op_type': 'Flatten', 'input_name': 'permute_4/transpose:0', 'output_name': 'flatten_4/Reshape:0', 'input_shape': [32, 7, 7], 'output_shape': [1568], 'activation': 'linear'}\n",
      "{'layer_name': 'dense_7', 'op_type': 'Dense', 'input_name': 'flatten_4/Reshape:0', 'output_name': 'dense_7/Relu:0', 'input_shape': [1568], 'output_shape': [100], 'units': 100, 'use_bias': True, 'activation': 'relu', 'weights_shape': [100, 1568]}\n",
      "{'layer_name': 'dense_8', 'op_type': 'Dense', 'input_name': 'dense_7/Relu:0', 'output_name': 'dense_8/BiasAdd:0', 'input_shape': [100], 'output_shape': [10], 'units': 10, 'use_bias': True, 'activation': 'linear', 'weights_shape': [10, 100]}\n",
      "export successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 保证所有数据能够显示，而不是用省略号表示，np.inf表示一个足够大的数\n",
    "# np.set_printoptions(threshold = np.inf) \n",
    "# 若想不以科学计数显示:\n",
    "# np.set_printoptions(suppress = True)\n",
    "# np.set_printoptions(linewidth = np.inf)\n",
    "\n",
    "import bson,re\n",
    "class Convert2MyTools:\n",
    "    dt = dict()\n",
    "    layers = []\n",
    "    def extract_con2d(self,data,layer):\n",
    "        obj = dict()\n",
    "        obj['layer_name'] = layer.name\n",
    "        obj['op_type'] = type(layer).__name__\n",
    "        obj['input_name'] = layer.input.name\n",
    "        obj['output_name'] = layer.output.name\n",
    "        obj['input_shape'] = list(map(int,layer.input_shape[1:4]))\n",
    "        obj['output_shape'] = list(map(int,layer.output_shape[1:4]))\n",
    "        obj['filters'] = layer.filters\n",
    "        obj['kernel_size'] = list(layer.kernel_size)\n",
    "        obj['strides'] = list(layer.strides)\n",
    "        #   tf中padding=='same'表示全填充,valid表示不填充，padding=0表示不填充,padding=1四周都填充\n",
    "        obj['padding'] = 1 if layer.padding.lower()=='same' else 0\n",
    "        obj['use_bias'] = layer.use_bias\n",
    "        obj['activation'] = layer.activation.__name__\n",
    "        weights = layer.get_weights()\n",
    "        ws = weights[0]\n",
    "        # tf中的纬度是h,w,inchannel,outchannel，在内部会根据批次转换成NHWC\n",
    "        # Julia中计算是H W IC OC，julia中reshape方向相反,,python纬度转换变成(outchannel,inchannel,w,h) 拉平导出\n",
    "        (filter_height, filter_width, in_channels, out_channels) = ws.shape\n",
    "        obj['filters_shape'] = list(map(int,ws.shape)) # 这里的过滤器shape是为了后续的weights转换所用\n",
    "        obj['filter_height'] = filter_height\n",
    "        obj['filter_width'] = filter_width\n",
    "        obj['in_channels'] = in_channels\n",
    "        obj['out_channels'] = out_channels\n",
    "        print(obj)\n",
    "        obj['weights'] = ws.transpose(3,2,1,0).flatten().tolist()\n",
    "        obj['bias'] = []\n",
    "        if layer.use_bias:\n",
    "            obj['bias'] = weights[1].tolist()\n",
    "        data.append(obj)\n",
    "\n",
    "    def extract_max_avg_pooling(self,data,layer):\n",
    "        obj = dict()\n",
    "        obj['layer_name'] = layer.name\n",
    "        obj['op_type'] = type(layer).__name__\n",
    "        obj['input_name'] = layer.input.name\n",
    "        obj['output_name'] = layer.output.name\n",
    "        obj['pool_size'] = list(layer.pool_size)\n",
    "        obj['strides'] = list(layer.strides)\n",
    "        obj['padding'] = 1 if layer.padding.lower()=='same' else 0\n",
    "        obj['input_shape'] = list(map(int,layer.input_shape[1:4]))\n",
    "        obj['output_shape'] = list(map(int,layer.output_shape[1:4]))\n",
    "        obj['activation'] = 'linear'\n",
    "        print(obj)\n",
    "        data.append(obj)\n",
    "        \n",
    "    def extract_global_max_avg_pooling(self,data,layer):\n",
    "        obj = dict()\n",
    "        obj['layer_name'] = layer.name\n",
    "        obj['op_type'] = type(layer).__name__\n",
    "        obj['input_name'] = layer.input.name\n",
    "        obj['output_name'] = layer.output.name\n",
    "        obj['input_shape'] = list(map(int,layer.input_shape[1:4]))\n",
    "        obj['output_shape'] = list(map(int,layer.output_shape[1:4]))\n",
    "        obj['activation'] = 'linear'\n",
    "        print(obj)\n",
    "        data.append(obj)\n",
    "\n",
    "    def extract_flatten(self,data,layer):\n",
    "        obj = dict()\n",
    "        obj['layer_name'] = layer.name\n",
    "        obj['op_type'] = type(layer).__name__\n",
    "        obj['input_name'] = layer.input.name\n",
    "        obj['output_name'] = layer.output.name\n",
    "        obj['input_shape'] = list(map(int,layer.input_shape[1:4]))\n",
    "        obj['output_shape'] = list(map(int,layer.output_shape[1:4]))\n",
    "        obj['activation'] = 'linear'\n",
    "        print(obj)\n",
    "        data.append(obj)\n",
    "    \n",
    "    def extract_permute(self,data,layer):\n",
    "        obj = dict()\n",
    "        obj['layer_name'] = layer.name\n",
    "        obj['op_type'] = type(layer).__name__\n",
    "        obj['input_name'] = layer.input.name\n",
    "        obj['output_name'] = layer.output.name\n",
    "        obj['input_shape'] = list(map(int,layer.input_shape[1:4]))\n",
    "        obj['output_shape'] = list(map(int,layer.output_shape[1:4]))\n",
    "        obj['dims'] = list(layer.dims)\n",
    "        obj['activation'] = 'linear'\n",
    "        print(obj)\n",
    "        data.append(obj)\n",
    "\n",
    "    def extract_dense(self,data,layer):\n",
    "        weights = layer.get_weights()\n",
    "        obj = dict()\n",
    "        obj['layer_name'] = layer.name\n",
    "        obj['op_type'] = type(layer).__name__\n",
    "        obj['input_name'] = layer.input.name\n",
    "        obj['output_name'] = layer.output.name\n",
    "        obj['input_shape'] = list(map(int,layer.input_shape[1:4]))\n",
    "        obj['output_shape'] = list(map(int,layer.output_shape[1:4]))\n",
    "        obj['units'] = layer.units\n",
    "        obj['use_bias'] = layer.use_bias\n",
    "#         if \"activation\" in layer.__dict__.items(): layer.has_key(\"activation\")\n",
    "        obj['activation'] = layer.activation.__name__\n",
    "        # 由于weights[0][:,i]是一列作为特征进行相乘，所以横向拉平后的数据在julia中对应的shape需要转置\n",
    "        obj['weights_shape'] = list(map(int,weights[0].shape))\n",
    "        obj['weights_shape'].reverse()\n",
    "        print(obj)\n",
    "        obj['weights'] = weights[0].flatten().tolist()\n",
    "        obj['bias'] = []\n",
    "        if layer.use_bias:\n",
    "            obj['bias'] = weights[1].tolist()\n",
    "        data.append(obj)\n",
    "     \n",
    "    def extract_zeropadding(self,data,layer):\n",
    "        obj = dict()\n",
    "        obj['layer_name'] = layer.name\n",
    "        obj['op_type'] = type(layer).__name__\n",
    "        obj['input_name'] = layer.input.name\n",
    "        obj['output_name'] = layer.output.name\n",
    "        obj['input_shape'] = list(map(int,layer.input_shape[1:4]))\n",
    "        obj['output_shape'] = list(map(int,layer.output_shape[1:4]))\n",
    "        obj['padding'] = np.array(layer.padding).flatten().tolist()\n",
    "        obj['activation'] = 'linear'\n",
    "        print(obj)\n",
    "        data.append(obj)\n",
    "    \n",
    "    def extract_batchnorm(self,data,layer):\n",
    "        weights = layer.get_weights()\n",
    "        obj = dict()\n",
    "        obj['layer_name'] = layer.name\n",
    "        obj['op_type'] = type(layer).__name__\n",
    "        obj['input_name'] = layer.input.name\n",
    "        obj['output_name'] = layer.output.name\n",
    "        obj['input_shape'] = list(map(int,layer.input_shape[1:4]))\n",
    "        obj['output_shape'] = list(map(int,layer.output_shape[1:4]))\n",
    "        obj['momentum'] = float(layer.momentum)\n",
    "        obj['epsilon'] = float(layer.epsilon)\n",
    "        obj['center'] = layer.center\n",
    "        obj['scale'] = layer.scale\n",
    "        print(obj)\n",
    "        obj['gamma'] = weights[0].tolist()\n",
    "        obj['beta'] = weights[1].tolist()\n",
    "        obj['moving_mean'] = weights[2].tolist()\n",
    "        obj['moving_variance'] = weights[3].tolist()\n",
    "        obj['activation'] = 'linear'\n",
    "        data.append(obj)\n",
    "        \n",
    "    def extract_activation(self,data,layer):\n",
    "        obj = dict()\n",
    "        obj['layer_name'] = layer.name\n",
    "        obj['op_type'] = type(layer).__name__\n",
    "        obj['input_name'] = layer.input.name\n",
    "        obj['output_name'] = layer.output.name\n",
    "        obj['input_shape'] = list(map(int,layer.input_shape[1:4]))\n",
    "        obj['output_shape'] = list(map(int,layer.output_shape[1:4]))\n",
    "        obj['activation'] = layer.activation.__name__\n",
    "        print(obj)\n",
    "        data.append(obj)\n",
    "        \n",
    "    def extract_add_or_concatenate(self,data,layer):\n",
    "        obj = dict()\n",
    "        obj['layer_name'] = layer.name\n",
    "        obj['op_type'] = type(layer).__name__\n",
    "        obj['output_name'] = layer.output.name\n",
    "        obj['output_shape'] = list(map(int,layer.output_shape[1:4]))\n",
    "        obj['activation'] = 'linear'\n",
    "        obj['input_names'] = []\n",
    "        obj['input_shapes'] = []\n",
    "        for _input in layer.input:\n",
    "            obj['input_names'].append(_input.name)\n",
    "            obj['input_shapes'].append(_input.shape.as_list()[1:4])\n",
    "        print(obj)\n",
    "        data.append(obj)\n",
    "        \n",
    "    def extract_dropout(self,data,layer):\n",
    "        obj = dict()\n",
    "        obj['layer_name'] = layer.name\n",
    "        obj['op_type'] = type(layer).__name__\n",
    "        obj['input_name'] = layer.input.name\n",
    "        obj['output_name'] = layer.output.name\n",
    "        obj['input_shape'] = list(map(int,layer.input_shape[1:4]))\n",
    "        obj['output_shape'] = list(map(int,layer.output_shape[1:4]))\n",
    "        obj['rate'] = float(layer.rate)\n",
    "        obj['activation'] = 'linear'\n",
    "        print(obj)\n",
    "        data.append(obj)\n",
    "        \n",
    "    global gd\n",
    "    def extract_model(self,model,path):\n",
    "        with open(path, 'wb') as fs:\n",
    "            data=[]\n",
    "            for index,layer in enumerate(model.layers):\n",
    "                layer_name = type(layer).__name__\n",
    "                if 'Conv2D' in layer_name:\n",
    "                    self.extract_con2d(data,layer)\n",
    "                elif 'Dense' in layer_name:\n",
    "                    self.extract_dense(data,layer)\n",
    "                elif 'Permute' in layer_name:\n",
    "                    self.extract_permute(data,layer)\n",
    "                elif 'Flatten' in layer_name:\n",
    "                    self.extract_flatten(data,layer)\n",
    "                elif 'Dropout' in layer_name:\n",
    "                    self.extract_dropout(data,layer)\n",
    "                elif 'Activation' in layer_name:\n",
    "                    self.extract_activation(data,layer)\n",
    "                elif 'ZeroPadding2D' in layer_name:\n",
    "                    self.extract_zeropadding(data,layer)\n",
    "                elif 'BatchNormalization' in layer_name:\n",
    "                    self.extract_batchnorm(data,layer) \n",
    "                elif layer_name in ['Add','Concatenate']:\n",
    "                    self.extract_add_or_concatenate(data,layer)\n",
    "                elif layer_name in ['MaxPooling2D','AveragePooling2D']:\n",
    "                    self.extract_max_avg_pooling(data,layer)\n",
    "                elif layer_name in ['GlobalMaxPooling2D','GlobalAveragePooling2D']:\n",
    "                    self.extract_global_max_avg_pooling(data,layer)\n",
    "                elif 'InputLayer' in layer_name:\n",
    "                    continue\n",
    "                else:\n",
    "                    print('not support layer',layer.name)\n",
    "            fs.write(bson.dumps({'data':data}))\n",
    "            print('export successfully!')\n",
    "\n",
    "\n",
    "model = load_model('../../Neurify/mnist/models/各模型转换/neurify_convnet_kolter.h5')\n",
    "\n",
    "a = Convert2MyTools()\n",
    "a.extract_model(model,'neurify_convnet_kolter.bson')\n",
    "\n",
    "\n",
    "# layer = model.layers[8]\n",
    "# print(type(layer).__name__)\n",
    "# print(layer.input_shape)\n",
    "# print(layer.output_shape)\n",
    "# weights = layer.get_weights()\n",
    "# ws = weights[0].transpose(3,2,1,0).flatten()\n",
    "# print(ws[:,:,0,0])\n",
    "# w_t = ws.transpose(3,2,1,0).flatten().tolist()\n",
    "# print(w_t)\n",
    "# a = re.sub(r'(]\\n)', '];',w_t)\n",
    "# a = re.sub(r'\\];[\\s]*\\[', '];[',a)\n",
    "# for layer in model.layers:\n",
    "#     print(layer.name)\n",
    "# for item in layer.__dict__.items():\n",
    "#     print(item)\n",
    "# with open('tt.bson', 'wb') as fs:\n",
    "#     p = dict()\n",
    "#     p[\"data\"]= ws.tolist()\n",
    "#     fs.write(bson.dumps(p))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T14:05:51.359261Z",
     "start_time": "2020-07-22T14:05:51.349253Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessing_data():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    print(x_train.shape)\n",
    "     # 归一化\n",
    "    x_train = x_train / 255\n",
    "    x_test = x_test / 255\n",
    "    x_train = x_train.reshape(60000,784) # 将图片摊平，变成向量\n",
    "    x_test = x_test.reshape(10000,784) # 对测试集进行同样的处理\n",
    "    # 标签激活\n",
    "    y_train = keras.utils.to_categorical(y_train, 10)\n",
    "    y_test = keras.utils.to_categorical(y_test, 10)\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T14:06:43.318084Z",
     "start_time": "2020-07-22T14:06:40.747423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                110       \n",
      "=================================================================\n",
      "Total params: 8,400\n",
      "Trainable params: 8,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=10,activation='relu',input_shape=(784,)))\n",
    "model.add(Dense(units=10,activation='relu'))\n",
    "model.add(Dense(units=10,activation='relu'))\n",
    "model.add(Dense(units=10,activation='relu'))\n",
    "model.add(Dense(units=10,activation='relu'))\n",
    "model.add(Dense(units=10,activation='softmax'))\n",
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T14:09:02.548814Z",
     "start_time": "2020-07-22T14:08:36.196503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "WARNING:tensorflow:From f:\\develop\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 1.2591 - accuracy: 0.5600 - val_loss: 0.5927 - val_accuracy: 0.8181\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.5137 - accuracy: 0.8454 - val_loss: 0.4514 - val_accuracy: 0.8665\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.4208 - accuracy: 0.8763 - val_loss: 0.3756 - val_accuracy: 0.8909\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.3679 - accuracy: 0.8928 - val_loss: 0.3443 - val_accuracy: 0.9040\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.3271 - accuracy: 0.9058 - val_loss: 0.3162 - val_accuracy: 0.9057\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2973 - accuracy: 0.9134 - val_loss: 0.2906 - val_accuracy: 0.9166\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2773 - accuracy: 0.9199 - val_loss: 0.2801 - val_accuracy: 0.9172\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2630 - accuracy: 0.9229 - val_loss: 0.2600 - val_accuracy: 0.9249\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2508 - accuracy: 0.9266 - val_loss: 0.2646 - val_accuracy: 0.9234\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2392 - accuracy: 0.9306 - val_loss: 0.2463 - val_accuracy: 0.9271\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.2290 - accuracy: 0.9321 - val_loss: 0.2509 - val_accuracy: 0.9269\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2209 - accuracy: 0.9356 - val_loss: 0.2318 - val_accuracy: 0.9324\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2157 - accuracy: 0.9369 - val_loss: 0.2297 - val_accuracy: 0.9313\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2083 - accuracy: 0.9384 - val_loss: 0.2352 - val_accuracy: 0.9315\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2052 - accuracy: 0.9399 - val_loss: 0.2279 - val_accuracy: 0.9354\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2005 - accuracy: 0.9406 - val_loss: 0.2223 - val_accuracy: 0.9351\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.1958 - accuracy: 0.9431 - val_loss: 0.2256 - val_accuracy: 0.9363\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1942 - accuracy: 0.9423 - val_loss: 0.2218 - val_accuracy: 0.9364\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.1917 - accuracy: 0.9432 - val_loss: 0.2273 - val_accuracy: 0.9367\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.1871 - accuracy: 0.9446 - val_loss: 0.2259 - val_accuracy: 0.9332\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1e449db2208>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = preprocessing_data()\n",
    "model.fit(x_train, y_train,batch_size=128, epochs=20, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "tf1.15",
   "language": "python",
   "name": "exp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
